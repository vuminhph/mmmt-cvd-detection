{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "from Const import *\n",
    "from helper_code import *\n",
    "from extract_features import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_and_labels(data_folder, verbose):\n",
    "    # Find data files.\n",
    "    if verbose >= 1:\n",
    "        tqdm.write('Finding data files...')\n",
    "\n",
    "    # Find the patient data files.\n",
    "    patient_files = find_patient_files(data_folder)\n",
    "    num_patient_files = len(patient_files)\n",
    "\n",
    "    if num_patient_files==0:\n",
    "        raise Exception('No data was provided.')\n",
    "\n",
    "    # Extract the features and labels.\n",
    "    if verbose >= 1:\n",
    "        tqdm.write('Extracting features and labels from the Challenge data...')\n",
    "\n",
    "    murmur_classes = ['Present', 'Unknown', 'Absent']\n",
    "    num_murmur_classes = len(murmur_classes)\n",
    "    outcome_classes = ['Abnormal', 'Normal']\n",
    "    num_outcome_classes = len(outcome_classes)\n",
    "\n",
    "    features = list()\n",
    "    murmurs = list()\n",
    "    outcomes = list()\n",
    "\n",
    "    # Create a executor with 4 workers\n",
    "    executor = ProcessPoolExecutor(max_workers=4)\n",
    "    inputs = []\n",
    "\n",
    "    for i in tqdm(range(num_patient_files)):\n",
    "        if verbose >= 2:\n",
    "            tqdm.write('    {}/{}...'.format(i+1, num_patient_files))\n",
    "\n",
    "        # Load the current patient data and recordings.\n",
    "        current_patient_data = load_patient_data(patient_files[i])\n",
    "        current_recordings = load_recordings(data_folder, current_patient_data)\n",
    "\n",
    "        # Extract features.\n",
    "        inputs.append((current_patient_data, current_recordings))\n",
    "        # current_features = get_features(current_patient_data, current_recordings)\n",
    "        # features.append(current_features)\n",
    "\n",
    "        # Extract labels and use one-hot encoding.\n",
    "        current_murmur = np.zeros(num_murmur_classes, dtype=int)\n",
    "        murmur = get_murmur(current_patient_data)\n",
    "        if murmur in murmur_classes:\n",
    "            j = murmur_classes.index(murmur)\n",
    "            current_murmur[j] = 1\n",
    "        murmurs.append(current_murmur)\n",
    "\n",
    "        current_outcome = np.zeros(num_outcome_classes, dtype=int)\n",
    "        outcome = get_outcome(current_patient_data)\n",
    "        if outcome in outcome_classes:\n",
    "            j = outcome_classes.index(outcome)\n",
    "            current_outcome[j] = 1\n",
    "        outcomes.append(current_outcome)\n",
    "\n",
    "    futures = [executor.submit(get_features, input[0], input[1]) for input in inputs]\n",
    "    for future in as_completed(futures):\n",
    "        current_features = future.result()\n",
    "        features.append(current_features)\n",
    "        \n",
    "    features = np.vstack(features)\n",
    "    murmurs = np.vstack(murmurs)\n",
    "    outcomes = np.vstack(outcomes)\n",
    "    \n",
    "    return (features, murmurs, outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding data files...\n",
      "Extracting features and labels from the Challenge data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da0684eff7a4dae8ee9a49cb538c2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1/743...\n",
      "    2/743...\n",
      "    3/743...\n",
      "    4/743...\n",
      "    5/743...\n",
      "    6/743...\n",
      "    7/743...\n",
      "    8/743...\n",
      "    9/743...\n",
      "    10/743...\n",
      "    11/743...\n",
      "    12/743...\n",
      "    13/743...\n",
      "    14/743...\n",
      "    15/743...\n",
      "    16/743...\n",
      "    17/743...\n",
      "    18/743...\n",
      "    19/743...\n",
      "    20/743...\n",
      "    21/743...\n",
      "    22/743...\n",
      "    23/743...\n",
      "    24/743...\n",
      "    25/743...\n",
      "    26/743...\n",
      "    27/743...\n",
      "    28/743...\n",
      "    29/743...\n",
      "    30/743...\n",
      "    31/743...\n",
      "    32/743...\n",
      "    33/743...\n",
      "    34/743...\n",
      "    35/743...\n",
      "    36/743...\n",
      "    37/743...\n",
      "    38/743...\n",
      "    39/743...\n",
      "    40/743...\n",
      "    41/743...\n",
      "    42/743...\n",
      "    43/743...\n",
      "    44/743...\n",
      "    45/743...\n",
      "    46/743...\n",
      "    47/743...\n",
      "    48/743...\n",
      "    49/743...\n",
      "    50/743...\n",
      "    51/743...\n",
      "    52/743...\n",
      "    53/743...\n",
      "    54/743...\n",
      "    55/743...\n",
      "    56/743...\n",
      "    57/743...\n",
      "    58/743...\n",
      "    59/743...\n",
      "    60/743...\n",
      "    61/743...\n",
      "    62/743...\n",
      "    63/743...\n",
      "    64/743...\n",
      "    65/743...\n",
      "    66/743...\n",
      "    67/743...\n",
      "    68/743...\n",
      "    69/743...\n",
      "    70/743...\n",
      "    71/743...\n",
      "    72/743...\n",
      "    73/743...\n",
      "    74/743...\n",
      "    75/743...\n",
      "    76/743...\n",
      "    77/743...\n",
      "    78/743...\n",
      "    79/743...\n",
      "    80/743...\n",
      "    81/743...\n",
      "    82/743...\n",
      "    83/743...\n",
      "    84/743...\n",
      "    85/743...\n",
      "    86/743...\n",
      "    87/743...\n",
      "    88/743...\n",
      "    89/743...\n",
      "    90/743...\n",
      "    91/743...\n",
      "    92/743...\n",
      "    93/743...\n",
      "    94/743...\n",
      "    95/743...\n",
      "    96/743...\n",
      "    97/743...\n",
      "    98/743...\n",
      "    99/743...\n",
      "    100/743...\n",
      "    101/743...\n",
      "    102/743...\n",
      "    103/743...\n",
      "    104/743...\n",
      "    105/743...\n",
      "    106/743...\n",
      "    107/743...\n",
      "    108/743...\n",
      "    109/743...\n",
      "    110/743...\n",
      "    111/743...\n",
      "    112/743...\n",
      "    113/743...\n",
      "    114/743...\n",
      "    115/743...\n",
      "    116/743...\n",
      "    117/743...\n",
      "    118/743...\n",
      "    119/743...\n",
      "    120/743...\n",
      "    121/743...\n",
      "    122/743...\n",
      "    123/743...\n",
      "    124/743...\n",
      "    125/743...\n",
      "    126/743...\n",
      "    127/743...\n",
      "    128/743...\n",
      "    129/743...\n",
      "    130/743...\n",
      "    131/743...\n",
      "    132/743...\n",
      "    133/743...\n",
      "    134/743...\n",
      "    135/743...\n",
      "    136/743...\n",
      "    137/743...\n",
      "    138/743...\n",
      "    139/743...\n",
      "    140/743...\n",
      "    141/743...\n",
      "    142/743...\n",
      "    143/743...\n",
      "    144/743...\n",
      "    145/743...\n",
      "    146/743...\n",
      "    147/743...\n",
      "    148/743...\n",
      "    149/743...\n",
      "    150/743...\n",
      "    151/743...\n",
      "    152/743...\n",
      "    153/743...\n",
      "    154/743...\n",
      "    155/743...\n",
      "    156/743...\n",
      "    157/743...\n",
      "    158/743...\n",
      "    159/743...\n",
      "    160/743...\n",
      "    161/743...\n",
      "    162/743...\n",
      "    163/743...\n",
      "    164/743...\n",
      "    165/743...\n",
      "    166/743...\n",
      "    167/743...\n",
      "    168/743...\n",
      "    169/743...\n",
      "    170/743...\n",
      "    171/743...\n",
      "    172/743...\n",
      "    173/743...\n",
      "    174/743...\n",
      "    175/743...\n",
      "    176/743...\n",
      "    177/743...\n",
      "    178/743...\n",
      "    179/743...\n",
      "    180/743...\n",
      "    181/743...\n",
      "    182/743...\n",
      "    183/743...\n",
      "    184/743...\n",
      "    185/743...\n",
      "    186/743...\n",
      "    187/743...\n",
      "    188/743...\n",
      "    189/743...\n",
      "    190/743...\n",
      "    191/743...\n",
      "    192/743...\n",
      "    193/743...\n",
      "    194/743...\n",
      "    195/743...\n",
      "    196/743...\n",
      "    197/743...\n",
      "    198/743...\n",
      "    199/743...\n",
      "    200/743...\n",
      "    201/743...\n",
      "    202/743...\n",
      "    203/743...\n",
      "    204/743...\n",
      "    205/743...\n",
      "    206/743...\n",
      "    207/743...\n",
      "    208/743...\n",
      "    209/743...\n",
      "    210/743...\n",
      "    211/743...\n",
      "    212/743...\n",
      "    213/743...\n",
      "    214/743...\n",
      "    215/743...\n",
      "    216/743...\n",
      "    217/743...\n",
      "    218/743...\n",
      "    219/743...\n",
      "    220/743...\n",
      "    221/743...\n",
      "    222/743...\n",
      "    223/743...\n",
      "    224/743...\n",
      "    225/743...\n",
      "    226/743...\n",
      "    227/743...\n",
      "    228/743...\n",
      "    229/743...\n",
      "    230/743...\n",
      "    231/743...\n",
      "    232/743...\n",
      "    233/743...\n",
      "    234/743...\n",
      "    235/743...\n",
      "    236/743...\n",
      "    237/743...\n",
      "    238/743...\n",
      "    239/743...\n",
      "    240/743...\n",
      "    241/743...\n",
      "    242/743...\n",
      "    243/743...\n",
      "    244/743...\n",
      "    245/743...\n",
      "    246/743...\n",
      "    247/743...\n",
      "    248/743...\n",
      "    249/743...\n",
      "    250/743...\n",
      "    251/743...\n",
      "    252/743...\n",
      "    253/743...\n",
      "    254/743...\n",
      "    255/743...\n",
      "    256/743...\n",
      "    257/743...\n",
      "    258/743...\n",
      "    259/743...\n",
      "    260/743...\n",
      "    261/743...\n",
      "    262/743...\n",
      "    263/743...\n",
      "    264/743...\n",
      "    265/743...\n",
      "    266/743...\n",
      "    267/743...\n",
      "    268/743...\n",
      "    269/743...\n",
      "    270/743...\n",
      "    271/743...\n",
      "    272/743...\n",
      "    273/743...\n",
      "    274/743...\n",
      "    275/743...\n",
      "    276/743...\n",
      "    277/743...\n",
      "    278/743...\n",
      "    279/743...\n",
      "    280/743...\n",
      "    281/743...\n",
      "    282/743...\n",
      "    283/743...\n",
      "    284/743...\n",
      "    285/743...\n",
      "    286/743...\n",
      "    287/743...\n",
      "    288/743...\n",
      "    289/743...\n",
      "    290/743...\n",
      "    291/743...\n",
      "    292/743...\n",
      "    293/743...\n",
      "    294/743...\n",
      "    295/743...\n",
      "    296/743...\n",
      "    297/743...\n",
      "    298/743...\n",
      "    299/743...\n",
      "    300/743...\n",
      "    301/743...\n",
      "    302/743...\n",
      "    303/743...\n",
      "    304/743...\n",
      "    305/743...\n",
      "    306/743...\n",
      "    307/743...\n",
      "    308/743...\n",
      "    309/743...\n",
      "    310/743...\n",
      "    311/743...\n",
      "    312/743...\n",
      "    313/743...\n",
      "    314/743...\n",
      "    315/743...\n",
      "    316/743...\n",
      "    317/743...\n",
      "    318/743...\n",
      "    319/743...\n",
      "    320/743...\n",
      "    321/743...\n",
      "    322/743...\n",
      "    323/743...\n",
      "    324/743...\n",
      "    325/743...\n",
      "    326/743...\n",
      "    327/743...\n",
      "    328/743...\n",
      "    329/743...\n",
      "    330/743...\n",
      "    331/743...\n",
      "    332/743...\n",
      "    333/743...\n",
      "    334/743...\n",
      "    335/743...\n",
      "    336/743...\n",
      "    337/743...\n",
      "    338/743...\n",
      "    339/743...\n",
      "    340/743...\n",
      "    341/743...\n",
      "    342/743...\n",
      "    343/743...\n",
      "    344/743...\n",
      "    345/743...\n",
      "    346/743...\n",
      "    347/743...\n",
      "    348/743...\n",
      "    349/743...\n",
      "    350/743...\n",
      "    351/743...\n",
      "    352/743...\n",
      "    353/743...\n",
      "    354/743...\n",
      "    355/743...\n",
      "    356/743...\n",
      "    357/743...\n",
      "    358/743...\n",
      "    359/743...\n",
      "    360/743...\n",
      "    361/743...\n",
      "    362/743...\n",
      "    363/743...\n",
      "    364/743...\n",
      "    365/743...\n",
      "    366/743...\n",
      "    367/743...\n",
      "    368/743...\n",
      "    369/743...\n",
      "    370/743...\n",
      "    371/743...\n",
      "    372/743...\n",
      "    373/743...\n",
      "    374/743...\n",
      "    375/743...\n",
      "    376/743...\n",
      "    377/743...\n",
      "    378/743...\n",
      "    379/743...\n",
      "    380/743...\n",
      "    381/743...\n",
      "    382/743...\n",
      "    383/743...\n",
      "    384/743...\n",
      "    385/743...\n",
      "    386/743...\n",
      "    387/743...\n",
      "    388/743...\n",
      "    389/743...\n",
      "    390/743...\n",
      "    391/743...\n",
      "    392/743...\n",
      "    393/743...\n",
      "    394/743...\n",
      "    395/743...\n",
      "    396/743...\n",
      "    397/743...\n",
      "    398/743...\n",
      "    399/743...\n",
      "    400/743...\n",
      "    401/743...\n",
      "    402/743...\n",
      "    403/743...\n",
      "    404/743...\n",
      "    405/743...\n",
      "    406/743...\n",
      "    407/743...\n",
      "    408/743...\n",
      "    409/743...\n",
      "    410/743...\n",
      "    411/743...\n",
      "    412/743...\n",
      "    413/743...\n",
      "    414/743...\n",
      "    415/743...\n",
      "    416/743...\n",
      "    417/743...\n",
      "    418/743...\n",
      "    419/743...\n",
      "    420/743...\n",
      "    421/743...\n",
      "    422/743...\n",
      "    423/743...\n",
      "    424/743...\n",
      "    425/743...\n",
      "    426/743...\n",
      "    427/743...\n",
      "    428/743...\n",
      "    429/743...\n",
      "    430/743...\n",
      "    431/743...\n",
      "    432/743...\n",
      "    433/743...\n",
      "    434/743...\n",
      "    435/743...\n",
      "    436/743...\n",
      "    437/743...\n",
      "    438/743...\n",
      "    439/743...\n",
      "    440/743...\n",
      "    441/743...\n",
      "    442/743...\n",
      "    443/743...\n",
      "    444/743...\n",
      "    445/743...\n",
      "    446/743...\n",
      "    447/743...\n",
      "    448/743...\n",
      "    449/743...\n",
      "    450/743...\n",
      "    451/743...\n",
      "    452/743...\n",
      "    453/743...\n",
      "    454/743...\n",
      "    455/743...\n",
      "    456/743...\n",
      "    457/743...\n",
      "    458/743...\n",
      "    459/743...\n",
      "    460/743...\n",
      "    461/743...\n",
      "    462/743...\n",
      "    463/743...\n",
      "    464/743...\n",
      "    465/743...\n",
      "    466/743...\n",
      "    467/743...\n",
      "    468/743...\n",
      "    469/743...\n",
      "    470/743...\n",
      "    471/743...\n",
      "    472/743...\n",
      "    473/743...\n",
      "    474/743...\n",
      "    475/743...\n",
      "    476/743...\n",
      "    477/743...\n",
      "    478/743...\n",
      "    479/743...\n",
      "    480/743...\n",
      "    481/743...\n",
      "    482/743...\n",
      "    483/743...\n",
      "    484/743...\n",
      "    485/743...\n",
      "    486/743...\n",
      "    487/743...\n",
      "    488/743...\n",
      "    489/743...\n",
      "    490/743...\n",
      "    491/743...\n",
      "    492/743...\n",
      "    493/743...\n",
      "    494/743...\n",
      "    495/743...\n",
      "    496/743...\n",
      "    497/743...\n",
      "    498/743...\n",
      "    499/743...\n",
      "    500/743...\n",
      "    501/743...\n",
      "    502/743...\n",
      "    503/743...\n",
      "    504/743...\n",
      "    505/743...\n",
      "    506/743...\n",
      "    507/743...\n",
      "    508/743...\n",
      "    509/743...\n",
      "    510/743...\n",
      "    511/743...\n",
      "    512/743...\n",
      "    513/743...\n",
      "    514/743...\n",
      "    515/743...\n",
      "    516/743...\n",
      "    517/743...\n",
      "    518/743...\n",
      "    519/743...\n",
      "    520/743...\n",
      "    521/743...\n",
      "    522/743...\n",
      "    523/743...\n",
      "    524/743...\n",
      "    525/743...\n",
      "    526/743...\n",
      "    527/743...\n",
      "    528/743...\n",
      "    529/743...\n",
      "    530/743...\n",
      "    531/743...\n",
      "    532/743...\n",
      "    533/743...\n",
      "    534/743...\n",
      "    535/743...\n",
      "    536/743...\n",
      "    537/743...\n",
      "    538/743...\n",
      "    539/743...\n",
      "    540/743...\n",
      "    541/743...\n",
      "    542/743...\n",
      "    543/743...\n",
      "    544/743...\n",
      "    545/743...\n",
      "    546/743...\n",
      "    547/743...\n",
      "    548/743...\n",
      "    549/743...\n",
      "    550/743...\n",
      "    551/743...\n",
      "    552/743...\n",
      "    553/743...\n",
      "    554/743...\n",
      "    555/743...\n",
      "    556/743...\n",
      "    557/743...\n",
      "    558/743...\n",
      "    559/743...\n",
      "    560/743...\n",
      "    561/743...\n",
      "    562/743...\n",
      "    563/743...\n",
      "    564/743...\n",
      "    565/743...\n",
      "    566/743...\n",
      "    567/743...\n",
      "    568/743...\n",
      "    569/743...\n",
      "    570/743...\n",
      "    571/743...\n",
      "    572/743...\n",
      "    573/743...\n",
      "    574/743...\n",
      "    575/743...\n",
      "    576/743...\n",
      "    577/743...\n",
      "    578/743...\n",
      "    579/743...\n",
      "    580/743...\n",
      "    581/743...\n",
      "    582/743...\n",
      "    583/743...\n",
      "    584/743...\n",
      "    585/743...\n",
      "    586/743...\n",
      "    587/743...\n",
      "    588/743...\n",
      "    589/743...\n",
      "    590/743...\n",
      "    591/743...\n",
      "    592/743...\n",
      "    593/743...\n",
      "    594/743...\n",
      "    595/743...\n",
      "    596/743...\n",
      "    597/743...\n",
      "    598/743...\n",
      "    599/743...\n",
      "    600/743...\n",
      "    601/743...\n",
      "    602/743...\n",
      "    603/743...\n",
      "    604/743...\n",
      "    605/743...\n",
      "    606/743...\n",
      "    607/743...\n",
      "    608/743...\n",
      "    609/743...\n",
      "    610/743...\n",
      "    611/743...\n",
      "    612/743...\n",
      "    613/743...\n",
      "    614/743...\n",
      "    615/743...\n",
      "    616/743...\n",
      "    617/743...\n",
      "    618/743...\n",
      "    619/743...\n",
      "    620/743...\n",
      "    621/743...\n",
      "    622/743...\n",
      "    623/743...\n",
      "    624/743...\n",
      "    625/743...\n",
      "    626/743...\n",
      "    627/743...\n",
      "    628/743...\n",
      "    629/743...\n",
      "    630/743...\n",
      "    631/743...\n",
      "    632/743...\n",
      "    633/743...\n",
      "    634/743...\n",
      "    635/743...\n",
      "    636/743...\n",
      "    637/743...\n",
      "    638/743...\n",
      "    639/743...\n",
      "    640/743...\n",
      "    641/743...\n",
      "    642/743...\n",
      "    643/743...\n",
      "    644/743...\n",
      "    645/743...\n",
      "    646/743...\n",
      "    647/743...\n",
      "    648/743...\n",
      "    649/743...\n",
      "    650/743...\n",
      "    651/743...\n",
      "    652/743...\n",
      "    653/743...\n",
      "    654/743...\n",
      "    655/743...\n",
      "    656/743...\n",
      "    657/743...\n",
      "    658/743...\n",
      "    659/743...\n",
      "    660/743...\n",
      "    661/743...\n",
      "    662/743...\n",
      "    663/743...\n",
      "    664/743...\n",
      "    665/743...\n",
      "    666/743...\n",
      "    667/743...\n",
      "    668/743...\n",
      "    669/743...\n",
      "    670/743...\n",
      "    671/743...\n",
      "    672/743...\n",
      "    673/743...\n",
      "    674/743...\n",
      "    675/743...\n",
      "    676/743...\n",
      "    677/743...\n",
      "    678/743...\n",
      "    679/743...\n",
      "    680/743...\n",
      "    681/743...\n",
      "    682/743...\n",
      "    683/743...\n",
      "    684/743...\n",
      "    685/743...\n",
      "    686/743...\n",
      "    687/743...\n",
      "    688/743...\n",
      "    689/743...\n",
      "    690/743...\n",
      "    691/743...\n",
      "    692/743...\n",
      "    693/743...\n",
      "    694/743...\n",
      "    695/743...\n",
      "    696/743...\n",
      "    697/743...\n",
      "    698/743...\n",
      "    699/743...\n",
      "    700/743...\n",
      "    701/743...\n",
      "    702/743...\n",
      "    703/743...\n",
      "    704/743...\n",
      "    705/743...\n",
      "    706/743...\n",
      "    707/743...\n",
      "    708/743...\n",
      "    709/743...\n",
      "    710/743...\n",
      "    711/743...\n",
      "    712/743...\n",
      "    713/743...\n",
      "    714/743...\n",
      "    715/743...\n",
      "    716/743...\n",
      "    717/743...\n",
      "    718/743...\n",
      "    719/743...\n",
      "    720/743...\n",
      "    721/743...\n",
      "    722/743...\n",
      "    723/743...\n",
      "    724/743...\n",
      "    725/743...\n",
      "    726/743...\n",
      "    727/743...\n",
      "    728/743...\n",
      "    729/743...\n",
      "    730/743...\n",
      "    731/743...\n",
      "    732/743...\n",
      "    733/743...\n",
      "    734/743...\n",
      "    735/743...\n",
      "    736/743...\n",
      "    737/743...\n",
      "    738/743...\n",
      "    739/743...\n",
      "    740/743...\n",
      "    741/743...\n",
      "    742/743...\n",
      "    743/743...\n"
     ]
    }
   ],
   "source": [
    "data_folder = \"C:/Users/lumin/Desktop/Work/20212/Data/circor-heart-sound/final/train\"\n",
    "verbose = 4\n",
    "\n",
    "features, murmurs, outcomes = extract_features_and_labels(data_folder, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer().fit(features)\n",
    "features = imputer.transform(features)\n",
    "murmur_classifier = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "outcome_classifier = RandomForestClassifier(random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for tuning\n",
    "random_grid = {\n",
    "    'n_estimators' : [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n",
    "    'criterion' : ['gini', 'entropy', 'log_loss'],\n",
    "    'max_features' : ['auto', 'sqrt'],\n",
    "    'max_depth' : [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "    'min_samples_split' : [2,5,10], \n",
    "    'min_samples_leaf' : [1,2,4], \n",
    "    'bootstrap' : [True, False], \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "murmur_random = RandomizedSearchCV(murmur_classifier, param_distributions=random_grid, n_iter=100, cv=5, verbose=verbose, random_state=42, n_jobs = -1)\n",
    "outcome_random = RandomizedSearchCV(outcome_classifier, param_distributions=random_grid, n_iter=100, cv=5, verbose=verbose, random_state=42, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "175 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.69317069 0.69584618        nan        nan        nan        nan\n",
      " 0.69183748 0.68911663        nan        nan        nan        nan\n",
      "        nan 0.68778342 0.71064756 0.69989117        nan 0.67566661\n",
      " 0.70392708        nan        nan 0.68240522 0.69316162        nan\n",
      " 0.71335933 0.67432432        nan 0.69452204 0.67701796 0.70526936\n",
      " 0.67566661 0.71199891 0.70527843 0.69584618        nan 0.69315255\n",
      " 0.707963   0.70931435        nan 0.69583711 0.6972066  0.69718846\n",
      " 0.71067477        nan 0.70527843 0.70662071 0.68643207        nan\n",
      "        nan        nan 0.69182841        nan        nan 0.6972066\n",
      "        nan        nan 0.68779249 0.70257573 0.67432432 0.68644114\n",
      " 0.68105387        nan 0.69451297 0.68375658        nan 0.70529657\n",
      " 0.68643207 0.70257573        nan 0.70529657 0.71200798 0.69721567\n",
      " 0.70393615 0.69853075        nan        nan 0.71200798 0.6998821\n",
      " 0.69182841 0.68914384        nan 0.7106657         nan        nan\n",
      " 0.69584618        nan 0.69451297 0.68508979 0.69451297 0.68645021\n",
      " 0.68645021 0.69719753 0.67969345 0.69855795 0.69584618 0.707963\n",
      " 0.69719753        nan        nan 0.68780156]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "175 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.52228369 0.53430074        nan        nan        nan        nan\n",
      " 0.52092327 0.53432795        nan        nan        nan        nan\n",
      "        nan 0.50745511 0.52898603 0.52626519        nan 0.50746418\n",
      " 0.52224742        nan        nan 0.51551787 0.52893162        nan\n",
      " 0.52895882 0.51015781        nan 0.51822057 0.51150009 0.52627426\n",
      " 0.50746418 0.52763468 0.52762561 0.52494105        nan 0.52222928\n",
      " 0.52760747 0.52894069        nan 0.53031018 0.52628333 0.52626519\n",
      " 0.52089606        nan 0.53031925 0.5275984  0.51958099        nan\n",
      "        nan        nan 0.52362597        nan        nan 0.51823871\n",
      "        nan        nan 0.52363504 0.52494105 0.51961727 0.51823871\n",
      " 0.51689643        nan 0.52494105 0.50070742        nan 0.52494105\n",
      " 0.51286051 0.52494105        nan 0.52494105 0.53166153 0.52358063\n",
      " 0.53435516 0.52351714        nan        nan 0.53166153 0.51955378\n",
      " 0.52362597 0.51822057        nan 0.52491384        nan        nan\n",
      " 0.52221114        nan 0.52494105 0.51823871 0.52896789 0.51823871\n",
      " 0.52361691 0.52493198 0.51823871 0.52896789 0.51686922 0.52765282\n",
      " 0.51551787        nan        nan 0.51417558]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(random_state=6789),\n",
       "                   n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'criterion': ['gini', 'entropy',\n",
       "                                                      'log_loss'],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000]},\n",
       "                   random_state=42, verbose=4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "murmur_random.fit(features, murmurs)\n",
    "outcome_random.fit(features, outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(criterion='entropy', max_depth=80, max_features='sqrt',\n",
      "                       min_samples_leaf=2, min_samples_split=10,\n",
      "                       n_estimators=1200, random_state=6789)\n",
      "0.7133593324868492\n"
     ]
    }
   ],
   "source": [
    "print(murmur_random.best_estimator_)\n",
    "print(murmur_random.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(criterion='entropy', max_depth=70, min_samples_split=5,\n",
      "                       n_estimators=800, random_state=6789)\n",
      "0.5343551605296571\n"
     ]
    }
   ],
   "source": [
    "print(outcome_random.best_estimator_)\n",
    "print(outcome_random.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for tuning\n",
    "murmur_param_grid = {\n",
    "    'n_estimators' : [800, 1000, 1200, 1400],\n",
    "    'criterion' : ['gini', 'entropy', 'log_loss'],\n",
    "    'max_features' : ['auto', 'sqrt'],\n",
    "    'max_depth' : [60,70,80,90],\n",
    "    'min_samples_split' : [8,10,12], \n",
    "    'min_samples_leaf' : [1,2,3], \n",
    "    'bootstrap' : [True, False], \n",
    "}\n",
    "\n",
    "outcome_param_grid = {\n",
    "    'n_estimators' : [800, 900, 700],\n",
    "    'criterion' : ['gini', 'entropy', 'log_loss'],\n",
    "    'max_features' : ['auto', 'sqrt'],\n",
    "    'max_depth' : [70,80,90, 60],\n",
    "    'min_samples_split' : [4,5,7], \n",
    "    'min_samples_leaf' : [4,5,6], \n",
    "    'bootstrap' : [True, False], \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "murmur_grid = GridSearchCV(murmur_classifier, param_grid=murmur_param_grid, cv=5, verbose=verbose, n_jobs = -1)\n",
    "outcome_grid = GridSearchCV(outcome_classifier, param_grid=outcome_param_grid, cv=5, verbose=verbose, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1728 candidates, totalling 8640 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "2880 fits failed out of a total of 8640.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1440 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1440 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.6945039  0.69585525 0.6972066  ...        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1296 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "2160 fits failed out of a total of 6480.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1080 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1080 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\lumin\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.52896789 0.53300381 0.53436423 ...        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=6789),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'bootstrap': [True, False],\n",
       "                         'criterion': ['gini', 'entropy', 'log_loss'],\n",
       "                         'max_depth': [70, 80, 90, 60],\n",
       "                         'max_features': ['auto', 'sqrt'],\n",
       "                         'min_samples_leaf': [4, 5, 6],\n",
       "                         'min_samples_split': [4, 5, 7],\n",
       "                         'n_estimators': [800, 900, 700]},\n",
       "             verbose=4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "murmur_grid.fit(features, murmurs)\n",
    "outcome_grid.fit(features, outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(criterion='entropy', max_depth=80, max_features='sqrt',\n",
      "                       min_samples_leaf=2, min_samples_split=10,\n",
      "                       n_estimators=1200, random_state=6789)\n",
      "0.7133593324868492\n"
     ]
    }
   ],
   "source": [
    "print(murmur_random.best_estimator_)\n",
    "print(murmur_random.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(max_depth=70, min_samples_leaf=4, min_samples_split=4,\n",
      "                       n_estimators=700, random_state=6789)\n",
      "0.5343642300018139\n"
     ]
    }
   ],
   "source": [
    "print(outcome_grid.best_estimator_)\n",
    "print(outcome_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kjj"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68e8aaa6874bd48bb187e47dd02de5530f22ef2f500911b0a3c153188765e172"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
